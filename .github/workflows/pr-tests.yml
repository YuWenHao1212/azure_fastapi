name: PR Tests

on:
  pull_request:
    branches:
      - staging  # åªé‡å° staging åˆ†æ”¯
    types: [opened, synchronize, reopened]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for proper diff

    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: ðŸ“¦ Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: ðŸ“š Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pre-commit

    - name: ðŸŽ¨ Run Ruff (Code Style)
      run: |
        echo "Running code style checks..."
        ruff check src/ tests/ --exclude=legacy,archive
        
        # Check if there are fixable issues
        if ! ruff check src/ tests/ --exclude=legacy,archive --fix --diff; then
          echo "::warning::Some issues can be auto-fixed with 'ruff check --fix'"
        fi

    - name: ðŸ“ Check YAML Files
      run: |
        echo "Validating YAML files..."
        python -c "
import yaml
import sys
from pathlib import Path

errors = []
for yaml_file in Path('src/prompts').rglob('*.yaml'):
    try:
        with open(yaml_file) as f:
            yaml.safe_load(f)
        print(f'âœ“ {yaml_file}')
    except Exception as e:
        errors.append(f'âœ— {yaml_file}: {e}')
        
if errors:
    print('\nErrors found:')
    for error in errors:
        print(error)
    sys.exit(1)
else:
    print('\nAll YAML files are valid!')
"

    - name: ðŸ§ª Run Unit Tests (Level 2)
      run: |
        echo "Running unit tests..."
        python -m pytest tests/unit/ -v --tb=short

    - name: ðŸ”— Run Integration Tests (Level 3)
      # PR to staging æ‡‰è©²åŸ·è¡Œ Level 3 æ¸¬è©¦
      env:
        # ä½¿ç”¨çœŸå¯¦ API æ†‘è­‰ï¼ˆæ ¹æ“š CLAUDE.md è¦æ±‚ï¼‰
        AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
        # GPT-4.1 mini (å¿…é ˆ - ä½¿ç”¨ä¸åŒçš„ç«¯é»ž)
        GPT41_MINI_JAPANEAST_API_KEY: ${{ secrets.GPT41_MINI_JAPANEAST_API_KEY }}
        GPT41_MINI_JAPANEAST_ENDPOINT: ${{ secrets.GPT41_MINI_JAPANEAST_ENDPOINT }}
        # åµŒå…¥æœå‹™
        AZURE_OPENAI_EMBEDDING_API_KEY: ${{ secrets.AZURE_OPENAI_EMBEDDING_API_KEY || secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_EMBEDDING_ENDPOINT: ${{ secrets.AZURE_OPENAI_EMBEDDING_ENDPOINT }}
        EMBEDDING_API_KEY: ${{ secrets.AZURE_OPENAI_EMBEDDING_API_KEY || secrets.AZURE_OPENAI_API_KEY }}
        EMBEDDING_ENDPOINT: ${{ secrets.AZURE_OPENAI_EMBEDDING_ENDPOINT }}
      run: |
        echo "Running integration tests with real APIs (Level 3)..."
        # å•Ÿå‹• API ä¼ºæœå™¨
        uvicorn src.main:app --port 8000 --log-level error > /tmp/api_server.log 2>&1 &
        API_PID=$!
        
        # ç­‰å¾…ä¼ºæœå™¨å•Ÿå‹•
        sleep 5
        
        # åŸ·è¡Œæ•´åˆæ¸¬è©¦
        if python -m pytest tests/integration/test_real_api_providers.py -v --tb=short; then
          echo "âœ… Integration tests passed"
        else
          echo "âŒ Integration tests failed"
          kill $API_PID 2>/dev/null || true
          exit 1
        fi
        
        # åœæ­¢ API ä¼ºæœå™¨
        kill $API_PID 2>/dev/null || true

    - name: ðŸ“Š Test Coverage Report
      if: github.event_name == 'pull_request'
      run: |
        # Run tests with coverage
        python -m pytest tests/unit/ --cov=src --cov-report=term-missing --cov-report=html
        
        # Generate coverage comment
        coverage_output=$(python -m pytest tests/unit/ --cov=src --cov-report=term-missing --quiet)
        echo "### ðŸ“Š Test Coverage Report" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "$coverage_output" | tail -n 20 >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY

    - name: ðŸ” Check for Large Files
      run: |
        # Find files larger than 1MB
        large_files=$(find . -type f -size +1M | grep -v ".git" | grep -v ".venv" | grep -v "__pycache__" || true)
        if [ -n "$large_files" ]; then
          echo "::warning::Large files detected (>1MB):"
          echo "$large_files"
        fi

    - name: ðŸ” Security Check
      run: |
        # Check for potential secrets
        if grep -r "api[_-]key.*=" src/ tests/ --include="*.py" | grep -v "os.getenv" | grep -v "settings\." | grep -v "mock"; then
          echo "::error::Potential hardcoded API keys found!"
          exit 1
        fi
        
        # Check for private keys
        if find . -name "*.pem" -o -name "*.key" | grep -v ".git"; then
          echo "::error::Private key files detected!"
          exit 1
        fi

    - name: ðŸ“ PR Summary
      if: always()
      run: |
        echo "## ðŸ” PR Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ job.status }}" == "success" ]; then
          echo "âœ… All tests passed!" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Some tests failed. Please check the logs." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“‹ Checklist" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] Code style (Ruff) passed" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] YAML files validated" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] Unit tests passed" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] Integration tests (Level 3) passed" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] No hardcoded secrets" >> $GITHUB_STEP_SUMMARY
        echo "- [ ] No large files" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“Œ **Note**: Level 4 (Azure Functions) æ¸¬è©¦éœ€è¦æ‰‹å‹•åŸ·è¡Œ" >> $GITHUB_STEP_SUMMARY