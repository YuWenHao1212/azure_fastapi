# Production vs Staging 效能比較與優化提案

## 執行摘要

本提案旨在系統性地比較 Production 和 Staging 環境的關鍵字提取 API 效能，識別效能差異，並提出具體的優化建議。

## 目標

1. **量化效能差異**：準確測量兩個環境的響應時間
2. **識別瓶頸**：找出造成效能差異的具體原因
3. **優化建議**：提供可行的效能改善方案
4. **建立基準**：為未來的效能監控建立基準線

## 測試環境

### Production 環境
- **URL**: `https://airesumeadvisor-fastapi-premium.azurewebsites.net`
- **Plan**: Premium Plan (EP1)
- **Location**: East Asia (Function App)
- **LLM**: GPT-4o-2 (Sweden Central) - 預設模型

### Staging 環境
- **URL**: `https://airesumeadvisor-fastapi-premium-staging.azurewebsites.net`
- **Plan**: Premium Plan (EP1) - Staging Slot
- **Location**: East Asia (Function App)
- **LLM**: GPT-4.1 mini (Japan East) - 新部署的優化模型

## 測試方法

### 1. 測試案例設計

#### 英文測試案例
- **Short (100字)**: 簡短職位描述
- **Medium (400字)**: 標準職位描述
- **Long (800字)**: 詳細職位描述，包含職責、要求、福利等

#### 中文測試案例
- **短篇 (100字)**: 簡短職位描述
- **中篇 (400字)**: 標準職位描述
- **長篇 (800字)**: 詳細職位描述，包含職責、要求、福利等

#### 測試策略
- 每個案例測試 5 次（避免快取影響）
- 使用不同的職位描述內容（避免快取命中）
- 總計：6 個案例 × 5 次 × 2 環境 = 60 次測試

### 2. 測試指標
- **Response Time (P50, P95, P99)**：API 整體響應時間
- **Processing Time**：API 內部處理時間
- **Network Latency**：網路延遲
- **Cold Start Time**：冷啟動時間
- **Throughput**：每秒處理請求數
- **Error Rate**：錯誤率

### 3. 測試工具
- **基礎測試**：curl + time 命令
- **負載測試**：Apache Bench (ab) 或 wrk
- **詳細分析**：自定義 Python 腳本
- **監控**：Application Insights

## 實施計畫

### Phase 1: 基準測試（1-2 小時）
1. 編寫測試腳本
2. 執行單一請求測試（各 100 次）
3. 記錄基本響應時間

### Phase 2: 負載測試（2-3 小時）
1. 並發測試（1, 5, 10, 20 concurrent requests）
2. 持續負載測試（5 分鐘）
3. 冷啟動測試

### Phase 3: 分析與優化（3-4 小時）
1. 分析 Application Insights 數據
2. 識別效能瓶頸
3. 提出優化建議

## 預期發現

根據 GPT-4.1 mini 整合方案的分析，我們預期看到：

### 1. 網路延遲大幅改善
- **Production (GPT-4o-2)**: East Asia ↔ Sweden = ~1600ms 往返
- **Staging (GPT-4.1 mini)**: East Asia ↔ Japan = ~300ms 往返
- **預期節省**: ~1300ms (81% 改善)

### 2. 時間分解（基於 Application Insights 數據）
| 階段 | Production (Sweden) | Staging (Japan) | 差異 |
|------|-------------------|-----------------|------|
| 網路往返 | 1600ms (27.1%) | 300ms | -1300ms |
| LLM 處理 | 2000ms (34.7%) | 2000ms | 0ms |
| Azure Function | 400ms (6.8%) | 400ms | 0ms |
| 其他處理 | 1900ms | 1900ms | 0ms |
| **總計** | **~5900ms** | **~4600ms** | **-1300ms** |

### 3. 可能的效能差異原因
1. **主要差異**：網路延遲（地理位置）
2. **次要因素**：
   - Staging slot 可能的資源限制
   - 冷啟動頻率差異
   - 快取策略差異

## 優化策略

### 短期優化（1-2 週）
1. **響應快取**
   - 實作 Redis 快取
   - 快取常見查詢

2. **並行處理**
   - 異步處理非關鍵任務
   - 批次處理優化

3. **Prompt 優化**
   - 減少 token 使用
   - 簡化 prompt 結構

### 中期優化（1 個月）
1. **架構優化**
   - 實作請求隊列
   - 負載均衡改善

2. **模型優化**
   - A/B 測試不同 LLM
   - 根據內容選擇模型

### 長期優化（2-3 個月）
1. **基礎設施升級**
   - 評估更高級的 Plan
   - 多區域部署

2. **演算法改進**
   - 混合式提取（規則 + LLM）
   - 自訓練模型

## 成功指標

1. **響應時間改善**
   - P95 < 3 秒（目前約 4-5 秒）
   - P50 < 2 秒

2. **成本效益**
   - 降低 20% API 調用成本
   - 提升 30% 吞吐量

3. **可靠性**
   - 錯誤率 < 0.1%
   - 可用性 > 99.9%

## 風險與緩解

1. **生產環境影響**
   - 使用節流避免影響正常服務
   - 在低峰期執行測試

2. **成本超支**
   - 設定測試預算上限
   - 監控 API 使用量

## 時間表

| 階段 | 時間 | 交付物 |
|------|------|--------|
| 測試腳本開發 | 2 小時 | 可重複使用的測試工具 |
| 執行測試 | 4 小時 | 原始測試數據 |
| 數據分析 | 3 小時 | 效能報告 |
| 優化建議 | 2 小時 | 優化路線圖 |
| **總計** | **11 小時** | **完整的效能分析報告** |

## 下一步行動

1. 審核並批准此提案
2. 開始實作測試腳本
3. 安排測試時間窗口
4. 執行測試並分析結果

---

提案撰寫時間：2025-07-27
負責人：Claude Code + WenHao