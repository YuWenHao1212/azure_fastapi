# 效能測試數據差異分析

**分析日期**: 2025-07-26  
**問題**: 三組測試數據不一致

## 數據差異總結

| 測試類型 | 測試方法 | GPT-4o-2 回應時間 | 備註 |
|---------|---------|-----------------|------|
| **初始詳細測試** | HTTP 請求到生產 API | 12,050ms | 完整的端到端測試 |
| **本地 vs 生產** | HTTP 請求 | 7,395ms (生產) / 8,729ms (本地) | HTTP 請求測試 |
| **無快取對比** | 直接呼叫 Service | 3,488ms | 繞過 HTTP/API 層 |

## 差異原因分析

### 1. 測試方法完全不同

**12秒測試 (test_keywords_detailed_performance_20250726.py)**
```python
# 使用 HTTP 請求到生產 API
async with httpx.AsyncClient(timeout=30.0) as client:
    response = await client.post(
        f"{API_URL}?code={API_KEY}",
        json=payload
    )
```
- ✅ 包含完整 HTTP 請求週期
- ✅ 包含 Azure Function 冷啟動
- ✅ 包含網路往返時間
- ✅ 包含 API Gateway 處理

**3.5秒測試 (test_gpt41_mini_no_cache_20250726.py)**
```python
# 直接呼叫內部 Service
from src.services.keyword_extraction import get_keyword_extraction_service
service = get_keyword_extraction_service()
result = await service.process({...})
```
- ❌ 沒有 HTTP 開銷
- ❌ 沒有網路延遲
- ❌ 沒有 Azure Function 開銷
- ❌ 沒有 API 序列化/反序列化

### 2. 時間分解對比

**完整 HTTP 請求 (12秒)**
```
總時間: 12,050ms
├── 客戶端到 Azure (台灣→香港): ~50ms
├── Azure Function 處理
│   ├── 冷啟動 (如果需要): 500-2000ms
│   ├── HTTP 請求解析: ~10ms
│   ├── 驗證和中間件: ~20ms
│   └── 序列化/反序列化: ~20ms
├── 實際服務處理
│   ├── 語言偵測: ~15ms
│   ├── OpenAI API 呼叫
│   │   ├── 網路延遲 (香港→瑞典): ~7,300ms
│   │   └── LLM 處理: ~4,696ms
│   └── 後處理: ~50ms
└── 回應返回: ~50ms
```

**直接 Service 呼叫 (3.5秒)**
```
總時間: 3,488ms
├── 語言偵測: ~5ms
├── OpenAI API 呼叫 (本地→瑞典)
│   ├── 網路延遲: ~1,000ms (本地網路較快?)
│   └── LLM 處理: ~2,400ms
└── 後處理: ~80ms
```

### 3. 為什麼初始 12秒無法複現？

1. **Azure Function 冷啟動**
   - 首次測試可能包含冷啟動（1-2秒）
   - 後續測試 Function 已經暖機

2. **網路條件變化**
   - 不同時間的網路延遲不同
   - 路由可能改變

3. **Azure 內部優化**
   - 連接池建立後更快
   - DNS 快取
   - TCP 連接重用

4. **測試數據差異**
   - 12秒測試使用較長的 JD（600+ 字）
   - 7秒測試使用較短的測試數據

## 正確的效能基準

### 生產環境實際效能
- **首次請求（冷啟動）**: 10-12 秒
- **暖機後請求**: 7-8 秒
- **快取命中**: < 100ms

### 各層級耗時
1. **HTTP/API 層**: 1-3 秒（含冷啟動）
2. **網路延遲（香港→瑞典）**: 4-7 秒
3. **LLM 處理**: 2-3 秒
4. **應用邏輯**: < 100ms

## 結論

1. **數據都是正確的**，只是測試方法不同：
   - 12秒 = 完整 HTTP 請求（含冷啟動）
   - 7秒 = HTTP 請求（暖機狀態）
   - 3.5秒 = 繞過 HTTP 層的內部測試

2. **真實用戶體驗**：
   - 首次使用：10-12秒
   - 一般使用：7-8秒
   - 快取命中：< 100ms

3. **GPT-4.1 mini 的實際效益**：
   - LLM 處理時間相近（約 2.5秒）
   - 主要優勢：成本降低 90%
   - 網路延遲改善：香港→日本 會比 香港→瑞典 快

## 建議

1. **效能測試標準化**：
   - 使用相同的測試方法
   - 明確標註測試條件
   - 區分冷啟動和暖機測試

2. **報告更新**：
   - 澄清不同測試方法的差異
   - 提供完整的時間分解
   - 強調成本效益而非純效能提升