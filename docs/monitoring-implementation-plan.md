# Index Calculation & Gap Analysis ç›£æŽ§å¯¦ä½œæ–¹æ¡ˆ

## æ–¹æ¡ˆ 1ï¼šå¢žå¼·ç¾æœ‰äº‹ä»¶è¿½è¹¤ï¼ˆæŽ¨è–¦ï¼‰

### 1.1 OpenAI Client Token è¿½è¹¤

ä¿®æ”¹ `openai_client.py` ä¾†è¿½è¹¤ token ä½¿ç”¨ï¼š

```python
# åœ¨ chat_completion æ–¹æ³•ä¸­
result = response.json()

# æå– token ä½¿ç”¨è³‡è¨Š
usage = result.get('usage', {})
tokens_info = {
    "prompt_tokens": usage.get('prompt_tokens', 0),
    "completion_tokens": usage.get('completion_tokens', 0),
    "total_tokens": usage.get('total_tokens', 0)
}

# è¨˜éŒ„åˆ°ç›£æŽ§
monitoring_service.track_event(
    "OpenAITokenUsage",
    {
        "deployment": self.deployment_id,
        "prompt_tokens": tokens_info["prompt_tokens"],
        "completion_tokens": tokens_info["completion_tokens"],
        "total_tokens": tokens_info["total_tokens"],
        "endpoint_type": "chat_completion",
        "messages_count": len(messages),
        "temperature": temperature,
        "max_tokens": max_tokens
    }
)
```

### 1.2 Index Calculation è©³ç´°ç›£æŽ§

åœ¨ `index_calculation.py` service ä¸­æ·»åŠ ï¼š

```python
# åœ¨ calculate_index æ–¹æ³•ä¸­
embedding_start = time.time()
resume_embedding = await self.embedding_client.create_embeddings([resume_text])
jd_embedding = await self.embedding_client.create_embeddings([jd_text])
embedding_time = time.time() - embedding_start

# è¿½è¹¤ embedding æ•ˆèƒ½
monitoring_service.track_event(
    "EmbeddingPerformance",
    {
        "operation": "index_calculation",
        "text_lengths": {
            "resume": len(resume_text),
            "job_description": len(jd_text)
        },
        "processing_time_ms": round(embedding_time * 1000, 2),
        "embeddings_count": 2
    }
)
```

### 1.3 Gap Analysis è©³ç´°ç›£æŽ§

åœ¨ `gap_analysis.py` service ä¸­æ·»åŠ ï¼š

```python
# åœ¨ analyze_gap æ–¹æ³•ä¸­
llm_start = time.time()
response = await openai_client.chat_completion(...)
llm_time = time.time() - llm_start

# å¾ž response æå– token ä½¿ç”¨
usage = response.get('usage', {})

monitoring_service.track_event(
    "GapAnalysisCompleted",
    {
        "language": language,
        "prompt_tokens": usage.get('prompt_tokens', 0),
        "completion_tokens": usage.get('completion_tokens', 0),
        "total_tokens": usage.get('total_tokens', 0),
        "processing_time_ms": round(llm_time * 1000, 2),
        "skill_queries_count": len(parsed_response.get('skill_queries', [])),
        "response_sections": {
            "strengths": len(parsed_response.get('strengths', [])),
            "gaps": len(parsed_response.get('gaps', [])),
            "improvements": len(parsed_response.get('improvements', []))
        }
    }
)
```

## æ–¹æ¡ˆ 2ï¼šå»ºç«‹å°ˆé–€çš„ Metrics Service

### 2.1 æ–°å¢ž MetricsCollector é¡žåˆ¥

```python
# src/services/metrics_collector.py
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

@dataclass
class APIMetrics:
    """API åŸ·è¡ŒæŒ‡æ¨™"""
    endpoint: str
    start_time: float
    end_time: Optional[float] = None
    
    # Token ä½¿ç”¨
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    
    # æ•ˆèƒ½æŒ‡æ¨™
    embedding_time_ms: float = 0
    llm_time_ms: float = 0
    total_time_ms: float = 0
    
    # çµæžœæŒ‡æ¨™
    similarity_percentage: Optional[int] = None
    keyword_coverage: Optional[int] = None
    gap_analysis_sections: Optional[dict] = None
    
    # éŒ¯èª¤è¿½è¹¤
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    
    def calculate_durations(self):
        """è¨ˆç®—å„é …æ™‚é–“"""
        if self.end_time:
            self.total_time_ms = (self.end_time - self.start_time) * 1000

class MetricsCollector:
    """æ”¶é›†å’Œå ±å‘Š API æŒ‡æ¨™"""
    
    def __init__(self, monitoring_service):
        self.monitoring_service = monitoring_service
        
    def track_api_metrics(self, metrics: APIMetrics):
        """ç™¼é€å®Œæ•´çš„ API æŒ‡æ¨™åˆ° Application Insights"""
        metrics.calculate_durations()
        
        self.monitoring_service.track_event(
            "APIMetricsComplete",
            {
                "endpoint": metrics.endpoint,
                "total_time_ms": round(metrics.total_time_ms, 2),
                "embedding_time_ms": round(metrics.embedding_time_ms, 2),
                "llm_time_ms": round(metrics.llm_time_ms, 2),
                "prompt_tokens": metrics.prompt_tokens,
                "completion_tokens": metrics.completion_tokens,
                "total_tokens": metrics.total_tokens,
                "similarity_percentage": metrics.similarity_percentage,
                "keyword_coverage": metrics.keyword_coverage,
                "has_error": metrics.error_type is not None,
                "error_type": metrics.error_type,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
```

## æ–¹æ¡ˆ 3ï¼šApplication Insights æŸ¥è©¢ç¯„ä¾‹

### 3.1 åŠŸèƒ½æ­£å¸¸æ€§æŸ¥è©¢

```kusto
// æª¢æŸ¥å„ API ç«¯é»žçš„æˆåŠŸçŽ‡
customEvents
| where timestamp > ago(1h)
| where name in ("IndexCalculationCompleted", "GapAnalysisCompleted", "APIMetricsComplete")
| summarize 
    TotalRequests = count(),
    SuccessfulRequests = countif(customDimensions.has_error == "false" or isempty(customDimensions.has_error)),
    FailedRequests = countif(customDimensions.has_error == "true")
    by Endpoint = tostring(customDimensions.endpoint)
| extend SuccessRate = round(100.0 * SuccessfulRequests / TotalRequests, 2)
| order by Endpoint
```

### 3.2 è™•ç†æ™‚é–“åˆ†æž

```kusto
// åˆ†æžå„ç«¯é»žçš„è™•ç†æ™‚é–“åˆ†ä½ˆ
customEvents
| where timestamp > ago(1h)
| where name == "APIMetricsComplete"
| extend 
    Endpoint = tostring(customDimensions.endpoint),
    TotalTime = todouble(customDimensions.total_time_ms),
    EmbeddingTime = todouble(customDimensions.embedding_time_ms),
    LLMTime = todouble(customDimensions.llm_time_ms)
| summarize 
    avg(TotalTime), 
    percentiles(TotalTime, 50, 90, 95, 99),
    avg(EmbeddingTime),
    avg(LLMTime)
    by Endpoint
```

### 3.3 Token æ¶ˆè€—åˆ†æž

```kusto
// åˆ†æž Token ä½¿ç”¨æƒ…æ³
customEvents
| where timestamp > ago(1h)
| where name in ("OpenAITokenUsage", "APIMetricsComplete")
| extend 
    PromptTokens = toint(customDimensions.prompt_tokens),
    CompletionTokens = toint(customDimensions.completion_tokens),
    TotalTokens = toint(customDimensions.total_tokens),
    Endpoint = tostring(customDimensions.endpoint)
| summarize 
    TotalPromptTokens = sum(PromptTokens),
    TotalCompletionTokens = sum(CompletionTokens),
    TotalTokensUsed = sum(TotalTokens),
    AvgPromptTokens = avg(PromptTokens),
    AvgCompletionTokens = avg(CompletionTokens),
    RequestCount = count()
    by Endpoint
| extend AvgTokensPerRequest = TotalTokensUsed / RequestCount
```

### 3.4 æˆæœ¬ä¼°ç®—æŸ¥è©¢

```kusto
// ä¼°ç®— API ä½¿ç”¨æˆæœ¬ (å‡è¨­æ¯ 1K tokens = $0.01)
customEvents
| where timestamp > ago(1d)
| where name == "APIMetricsComplete"
| extend 
    TotalTokens = toint(customDimensions.total_tokens),
    Endpoint = tostring(customDimensions.endpoint)
| summarize 
    TotalTokensUsed = sum(TotalTokens),
    RequestCount = count()
    by bin(timestamp, 1h), Endpoint
| extend EstimatedCostUSD = round(TotalTokensUsed * 0.01 / 1000, 4)
| order by timestamp desc
```

## å¯¦ä½œå»ºè­°

1. **å„ªå…ˆå¯¦ä½œæ–¹æ¡ˆ 1**ï¼šæœ€å°æ”¹å‹•ï¼Œç«‹å³å¯ç”¨
2. **é€æ­¥å¢žåŠ  MetricsCollector**ï¼šæ›´çµæ§‹åŒ–çš„æ•¸æ“šæ”¶é›†
3. **è¨­ç½® Azure Monitor Workbook**ï¼šè¦–è¦ºåŒ–é—œéµæŒ‡æ¨™

## ç›£æŽ§é‡é»žæŒ‡æ¨™

### å¿…è¦æŒ‡æ¨™
- âœ… API æˆåŠŸ/å¤±æ•—çŽ‡
- âœ… ç«¯åˆ°ç«¯è™•ç†æ™‚é–“
- âœ… Token ä½¿ç”¨é‡ï¼ˆè¼¸å…¥/è¼¸å‡ºï¼‰
- âœ… å„æ­¥é©Ÿè€—æ™‚ï¼ˆembedding, LLM, parsingï¼‰

### å»ºè­°æŒ‡æ¨™
- ðŸ“Š ç›¸ä¼¼åº¦åˆ†æ•¸åˆ†ä½ˆ
- ðŸ“Š é—œéµè©žè¦†è“‹çŽ‡åˆ†ä½ˆ
- ðŸ“Š æŠ€èƒ½æŸ¥è©¢æ•¸é‡
- ðŸ“Š éŒ¯èª¤é¡žåž‹åˆ†ä½ˆ

### å‘Šè­¦è¨­ç½®
- ðŸš¨ éŒ¯èª¤çŽ‡ > 5%
- ðŸš¨ å¹³å‡è™•ç†æ™‚é–“ > 5 ç§’
- ðŸš¨ Token ä½¿ç”¨ç•°å¸¸é«˜
- ðŸš¨ é€£çºŒå¤±æ•— > 3 æ¬¡