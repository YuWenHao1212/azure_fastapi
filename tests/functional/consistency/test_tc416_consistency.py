#!/usr/bin/env python3
"""
TC-416: é—œéµå­—æå–ä¸€è‡´æ€§æ¸¬è©¦ï¼ˆå¤šèªè¨€æ”¯æ´ï¼‰
æ ¸å¿ƒ KPI æ¸¬è©¦ - è©•ä¼° AI é—œéµå­—æå–çš„ç©©å®šæ€§å’Œä¸€è‡´æ€§

ç”¨æ³•: 
    python test_tc416_consistency.py                    # ä½¿ç”¨é è¨­ JD
    python test_tc416_consistency.py --jd-file job.txt  # ä½¿ç”¨è‡ªè¨‚ JD æª”æ¡ˆ
    python test_tc416_consistency.py --jd "è·ä½æè¿°å…§å®¹" # ç›´æ¥æä¾› JD å…§å®¹
    python test_tc416_consistency.py --iterations 20    # è‡ªè¨‚æ¸¬è©¦æ¬¡æ•¸

åŠŸèƒ½:
- æ”¯æ´ä»»ä½•èªè¨€çš„ JDï¼ˆè‡ªå‹•åµæ¸¬èªè¨€ï¼‰
- æ ¹æ“šåµæ¸¬åˆ°çš„èªè¨€è‡ªå‹•é¸æ“‡å°æ‡‰çš„æœ€æ–° prompt ç‰ˆæœ¬
- åˆ†æé—œéµå­—é‡è¤‡ç‡å’Œ Jaccard ç›¸ä¼¼åº¦
- é©—è­‰èªè¨€æª¢æ¸¬ä¸€è‡´æ€§
- ç”Ÿæˆè©³ç´°çš„ä¸€è‡´æ€§å ±å‘Š
- è¨ˆç®— KPI æŒ‡æ¨™

âš ï¸  æ³¨æ„:
- éœ€è¦çœŸå¯¦ OpenAI API é‡‘é‘°
- åŸ·è¡Œæ™‚é–“: 5-10 åˆ†é˜ï¼ˆ50æ¬¡æ¸¬è©¦ï¼‰
- æœƒç”¢ç”Ÿ API ä½¿ç”¨è²»ç”¨
- éœ€è¦ç©©å®šç¶²è·¯é€£ç·š

Author: Claude Code
Date: 2025-07-04
"""
import asyncio
import json
import time
import sys
import os
import argparse
from typing import Tuple
from datetime import datetime
from pathlib import Path
from collections import Counter

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Set up environment
os.environ['PYTHONPATH'] = str(project_root) + ":" + os.environ.get('PYTHONPATH', '')

# Import services
from src.services.keyword_extraction_v2 import KeywordExtractionServiceV2

# é è¨­æ¸¬è©¦ JDï¼ˆä¸­æ–‡ï¼‰
DEFAULT_JD = """
Project Manager
Responsible for wafer outsourcing and cost negotiation, including:
Competitive Price: RFQ, price negotiation, cost prediction, cost reduction initiatives, CA.Capacity: long-term reservation, short-term fulfillment, acting as a bridge for internal and external parties to reach goals. Contract: NDA and short-term agreements. Project management (e.g. procurement system)

**Requirement

**Proactive working attitude and good communication skills, with negotiation capability being a plus.Experience in handling wafer sourcing or procurement. Project management and business acumen.
"""

class TC416ConsistencyTest:
    """TC-416: å¤šèªè¨€é—œéµå­—æå–ä¸€è‡´æ€§æ¸¬è©¦"""
    
    def __init__(self, iterations: int = 50, delay_seconds: float = 1.0):
        # é—œé–‰å¿«å–ä»¥æ¸¬è©¦çœŸå¯¦çš„ AI ä¸€è‡´æ€§ï¼ˆè€Œéå¿«å–ä¸€è‡´æ€§ï¼‰
        self.keyword_service = KeywordExtractionServiceV2(enable_cache=False)
        self.iterations = iterations
        self.delay_seconds = delay_seconds
        
        # ç¢ºèªå¿«å–æ˜¯å¦çœŸçš„é—œé–‰
        cache_info = self.keyword_service.get_cache_info()
        print(f"âš™ï¸  å¿«å–è¨­å®š: {'é—œé–‰' if not cache_info['enabled'] else 'é–‹å•Ÿ'}")
        print(f"   å¿«å–å¤§å°: {cache_info['total_entries']}")
        
        # æ¸…ç©ºä»»ä½•ç¾æœ‰å¿«å–
        if cache_info['total_entries'] > 0:
            self.keyword_service.clear_cache()
            print(f"   å·²æ¸…ç©º {cache_info['total_entries']} å€‹å¿«å–é …ç›®")
        
        self.test_results = []
        self.start_time = None
        self.end_time = None
        self.test_jd = None
        self.detected_language = None
        self.prompt_version_used = None
        
    def set_test_jd(self, jd: str):
        """è¨­å®šæ¸¬è©¦ç”¨çš„ JD"""
        self.test_jd = jd.strip()
        
    def print_header(self):
        """æ‰“å°æ¸¬è©¦æ¨™é¡Œ"""
        print("=" * 70)
        print(f"ğŸ§ª TC-416: é—œéµå­—æå–ä¸€è‡´æ€§æ¸¬è©¦ ({self.iterations}æ¬¡)")
        print("ğŸ“Š æ ¸å¿ƒ KPI æ¸¬è©¦ - AI ç©©å®šæ€§å’Œä¸€è‡´æ€§è©•ä¼°")
        print("=" * 70)
        print(f"å°ˆæ¡ˆç›®éŒ„: {project_root}")
        print(f"Python ç‰ˆæœ¬: {sys.version.split()[0]}")
        print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"JD é•·åº¦: {len(self.test_jd)} å­—ç¬¦")
        print("=" * 70)
        print(f"âš ï¸  æ³¨æ„: æ­¤æ¸¬è©¦å°‡é€²è¡Œ {self.iterations} æ¬¡çœŸå¯¦ AI å‘¼å«")
        # é ä¼°æ™‚é–“åŒ…å« API èª¿ç”¨æ™‚é–“ï¼ˆç´„ 2 ç§’ï¼‰å’Œå»¶é²æ™‚é–“
        estimated_seconds = self.iterations * (2 + self.delay_seconds)
        estimated_minutes = estimated_seconds / 60
        print(f"â±ï¸  é ä¼°åŸ·è¡Œæ™‚é–“: {estimated_minutes:.1f} åˆ†é˜ (åŒ…å« {self.delay_seconds} ç§’å»¶é²)")
        print("ğŸ’° æœƒç”¢ç”Ÿ OpenAI API ä½¿ç”¨è²»ç”¨")
        print("=" * 70)
        
    def log_progress(self, iteration, total, result=None):
        """è¨˜éŒ„é€²åº¦"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        progress = f"[{timestamp}] é€²åº¦: {iteration:2d}/{total}"
        
        if result:
            keywords_count = len(result.get("keywords", []))
            lang = result.get("detected_language", "æœªçŸ¥")
            confidence = result.get("confidence_score", 0)
            cache_hit = result.get("cache_hit", False)
            prompt_version = result.get("prompt_version", "N/A")
            progress += f" | é—œéµå­—: {keywords_count:2d} | èªè¨€: {lang} | ç‰ˆæœ¬: {prompt_version} | å¿«å–: {'æ˜¯' if cache_hit else 'å¦'}"
        
        print(progress)
    
    async def detect_language_and_version(self):
        """åµæ¸¬èªè¨€ä¸¦æ±ºå®šä½¿ç”¨çš„ prompt ç‰ˆæœ¬"""
        print("\nğŸŒ åµæ¸¬ JD èªè¨€...")
        
        # ä½¿ç”¨ç¬¬ä¸€æ¬¡æ¸¬è©¦ä¾†åµæ¸¬èªè¨€
        result = await self.keyword_service.process({
            "job_description": self.test_jd,
            "max_keywords": 16,
            "include_standardization": True,
            "use_multi_round_validation": True,
            "language": "auto"  # è‡ªå‹•åµæ¸¬èªè¨€
        })
        
        self.detected_language = result.get("detected_language", "unknown")
        self.prompt_version_used = result.get("prompt_version", "latest")
        
        print(f"   åµæ¸¬åˆ°èªè¨€: {self.detected_language}")
        print(f"   ä½¿ç”¨ Prompt ç‰ˆæœ¬: {self.prompt_version_used}")
        
        # ç²å–å¯ç”¨çš„ prompt ç‰ˆæœ¬è³‡è¨Š
        print(f"   æ”¯æ´çš„ prompt ç‰ˆæœ¬: v1.0.0, v1.2.0, v1.3.0, v1.4.0")
        
        return result
    
    async def run_consistency_test(self):
        """åŸ·è¡Œä¸€è‡´æ€§æ¸¬è©¦"""
        self.print_header()
        
        # é¦–å…ˆåµæ¸¬èªè¨€
        first_result = await self.detect_language_and_version()
        
        print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œ {self.iterations} æ¬¡ä¸€è‡´æ€§æ¸¬è©¦...")
        print("-" * 50)
        
        self.start_time = time.time()
        all_keywords = []
        
        # å°‡ç¬¬ä¸€æ¬¡çµæœåŠ å…¥
        self.test_results.append({
            "iteration": 1,
            "keywords": first_result.get("keywords", []),
            "keyword_count": len(first_result.get("keywords", [])),
            "detected_language": first_result.get("detected_language", ""),
            "confidence_score": first_result.get("confidence_score", 0),
            "extraction_method": first_result.get("extraction_method", ""),
            "processing_time_ms": 0,  # ç¬¬ä¸€æ¬¡æ²’æœ‰è¨ˆæ™‚
            "cache_hit": first_result.get("cache_hit", False),
            "intersection_stats": first_result.get("intersection_stats", {}),
            "llm_config_used": first_result.get("llm_config_used", {}),
            "prompt_version": first_result.get("prompt_version", ""),
            "timestamp": datetime.now().isoformat()
        })
        
        # é¡¯ç¤º LLM é…ç½®
        llm_config = first_result.get("llm_config_used", {})
        if llm_config:
            print(f"\nğŸ“‹ LLM é…ç½®:")
            print(f"   Temperature: {llm_config.get('temperature', 'N/A')}")
            print(f"   Seed: {llm_config.get('seed', 'N/A')}")
            print(f"   Top-p: {llm_config.get('top_p', 'N/A')}")
            print(f"   Max tokens: {llm_config.get('max_tokens', 'N/A')}")
            print()
        
        self.log_progress(1, self.iterations, first_result)
        
        # åŸ·è¡Œå‰©é¤˜çš„æ¸¬è©¦
        for i in range(2, self.iterations + 1):
            try:
                iteration_start = time.time()
                
                # èª¿ç”¨é—œéµå­—æå–æœå‹™ï¼Œä½¿ç”¨åµæ¸¬åˆ°çš„èªè¨€å°æ‡‰çš„ prompt
                result = await self.keyword_service.process({
                    "job_description": self.test_jd,
                    "max_keywords": 16,
                    "include_standardization": True,
                    "use_multi_round_validation": True,
                    "prompt_version": self.prompt_version_used,  # ä½¿ç”¨åµæ¸¬åˆ°çš„ç‰ˆæœ¬
                    "language": self.detected_language  # ä½¿ç”¨åµæ¸¬åˆ°çš„èªè¨€
                })
                
                processing_time = int((time.time() - iteration_start) * 1000)
                keywords = result.get("keywords", [])
                
                # è¨˜éŒ„æ­¤æ¬¡æ¸¬è©¦çµæœ
                iteration_result = {
                    "iteration": i,
                    "keywords": keywords,
                    "keyword_count": len(keywords),
                    "detected_language": result.get("detected_language", ""),
                    "confidence_score": result.get("confidence_score", 0),
                    "extraction_method": result.get("extraction_method", ""),
                    "processing_time_ms": processing_time,
                    "cache_hit": result.get("cache_hit", False),
                    "intersection_stats": result.get("intersection_stats", {}),
                    "llm_config_used": result.get("llm_config_used", {}),
                    "prompt_version": result.get("prompt_version", ""),
                    "timestamp": datetime.now().isoformat()
                }
                
                self.test_results.append(iteration_result)
                all_keywords.extend(keywords)
                
                # æ¯ 10 æ¬¡æˆ–å‰ 5 æ¬¡é¡¯ç¤ºé€²åº¦
                if i % 10 == 0 or i <= 5:
                    self.log_progress(i, self.iterations, result)
                
                # åŠ å…¥å»¶é²ä»¥é¿å… rate limit (é™¤äº†æœ€å¾Œä¸€æ¬¡)
                if i < self.iterations:
                    await asyncio.sleep(self.delay_seconds)
                
            except Exception as e:
                error_result = {
                    "iteration": i,
                    "error": str(e),
                    "failed": True,
                    "timestamp": datetime.now().isoformat()
                }
                self.test_results.append(error_result)
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ ç¬¬ {i} æ¬¡æ¸¬è©¦å¤±æ•—: {str(e)}")
        
        self.end_time = time.time()
        total_time = self.end_time - self.start_time
        
        print(f"\nâœ… {self.iterations} æ¬¡æ¸¬è©¦å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.1f} ç§’")
        print("-" * 50)
        
        # åˆ†æçµæœ
        await self.analyze_results()
    
    async def analyze_results(self):
        """åˆ†æä¸€è‡´æ€§çµæœ"""
        print("ğŸ” åˆ†æä¸€è‡´æ€§çµæœ...")
        print("-" * 40)
        
        # éæ¿¾æˆåŠŸçš„æ¸¬è©¦
        successful_results = [r for r in self.test_results if not r.get("failed", False)]
        failed_count = len(self.test_results) - len(successful_results)
        
        # æª¢æŸ¥å¿«å–å‘½ä¸­æƒ…æ³
        cache_hits = sum(1 for r in successful_results if r.get("cache_hit", False))
        
        print(f"æˆåŠŸæ¸¬è©¦: {len(successful_results)}/{self.iterations}")
        print(f"å¤±æ•—æ¸¬è©¦: {failed_count}/{self.iterations}")
        print(f"å¿«å–å‘½ä¸­: {cache_hits}/{len(successful_results)} {'âš ï¸  å¿«å–è¢«å•Ÿç”¨ï¼' if cache_hits > 0 else ''}")
        
        if len(successful_results) < self.iterations * 0.6:
            print(f"âŒ æˆåŠŸæ¸¬è©¦æ•¸é‡ä¸è¶³ï¼ˆ< {int(self.iterations * 0.6)}ï¼‰ï¼Œç„¡æ³•é€²è¡Œå¯é çš„ä¸€è‡´æ€§åˆ†æ")
            return
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰çµæœéƒ½ç›¸åŒï¼ˆå¯èƒ½æ˜¯å›ºå®š seed çš„å•é¡Œï¼‰
        if len(successful_results) > 1:
            first_keywords = set(successful_results[0]["keywords"])
            all_same = all(set(r["keywords"]) == first_keywords for r in successful_results[1:])
            if all_same:
                print("\nâš ï¸  è­¦å‘Šï¼šæ‰€æœ‰æ¸¬è©¦çµæœå®Œå…¨ç›¸åŒï¼")
                print("   å¯èƒ½åŸå› ï¼š")
                print("   1. Azure OpenAI å¯èƒ½ä¸æ”¯æ´ seed åƒæ•¸")
                print("   2. Temperature=0 + top_p=0.1 å°è‡´å®Œå…¨ç¢ºå®šæ€§è¼¸å‡º")
                print(f"   3. ç•¶å‰é…ç½®ï¼š{self.prompt_version_used}")
                print("   â†’ é€™ä¸æ˜¯çœŸå¯¦çš„ AI ä¸€è‡´æ€§æ¸¬è©¦ï¼")
                print("\n   å»ºè­°ï¼š")
                print("   1. æé«˜ temperature åˆ° 0.1-0.3")
                print("   2. æé«˜ top_p åˆ° 0.8-0.95")
                print("   3. ä½¿ç”¨ä¸åŒçš„ prompt ç‰ˆæœ¬é€²è¡Œæ¸¬è©¦")
        
        # 1. èªè¨€æª¢æ¸¬ä¸€è‡´æ€§
        languages = [r["detected_language"] for r in successful_results]
        language_counter = Counter(languages)
        most_common_lang = language_counter.most_common(1)[0][0]
        language_consistency = language_counter[most_common_lang] / len(languages)
        
        # 2. é—œéµå­—æ•¸é‡åˆ†æ
        keyword_counts = [r["keyword_count"] for r in successful_results]
        avg_keyword_count = sum(keyword_counts) / len(keyword_counts)
        min_keywords = min(keyword_counts)
        max_keywords = max(keyword_counts)
        
        # 3. è¨ˆç®—å®Œæ•´çš„ä¸€è‡´æ€§æŒ‡æ¨™
        # 3.1 è¨ˆç®—æ‰€æœ‰é…å°çš„ Jaccard ç›¸ä¼¼åº¦
        jaccard_similarities = []
        keyword_combinations = []
        identical_pairs = 0
        
        for result in successful_results:
            combo = tuple(sorted(result["keywords"]))
            keyword_combinations.append(combo)
        
        # æ¯”è¼ƒæ‰€æœ‰é…å°ï¼ˆä¸åªæ˜¯ç›¸é„°çš„ï¼‰
        total_pairs = len(successful_results) * (len(successful_results) - 1) // 2
        
        for i in range(len(successful_results)):
            for j in range(i + 1, len(successful_results)):
                set1 = set(successful_results[i]["keywords"])
                set2 = set(successful_results[j]["keywords"])
                intersection = len(set1.intersection(set2))
                union = len(set1.union(set2))
                jaccard = intersection / union if union > 0 else 0
                jaccard_similarities.append(jaccard)
                
                # æª¢æŸ¥æ˜¯å¦å®Œå…¨ç›¸åŒ
                if keyword_combinations[i] == keyword_combinations[j]:
                    identical_pairs += 1
        
        avg_jaccard = sum(jaccard_similarities) / len(jaccard_similarities) if jaccard_similarities else 0
        exact_match_rate = identical_pairs / total_pairs if total_pairs > 0 else 0
        
        # 3.2 è¨ˆç®— 95% ä¿¡å¿ƒå€é–“
        import math
        p = exact_match_rate
        n = total_pairs
        
        if p > 0 and p < 1 and n > 0:
            se = math.sqrt(p * (1 - p) / n)
            z = 1.96  # 95% ä¿¡å¿ƒæ°´æº–
            ci_lower = max(0, p - z * se)
            ci_upper = min(1, p + z * se)
        else:
            se = 0
            ci_lower = p
            ci_upper = p
        
        # 4. é—œéµå­—é‡è¤‡ç‡åˆ†æ
        all_keywords = []
        for result in successful_results:
            all_keywords.extend(result["keywords"])
        
        unique_keywords = list(set(all_keywords))
        total_keywords = len(all_keywords)
        unique_count = len(unique_keywords)
        repetition_rate = (total_keywords - unique_count) / total_keywords if total_keywords > 0 else 0
        
        # 5. æœ€é«˜é »é—œéµå­—
        keyword_frequency = {}
        for keyword in all_keywords:
            keyword_frequency[keyword] = keyword_frequency.get(keyword, 0) + 1
        
        top_keywords = sorted(keyword_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # 6. åŸ·è¡Œæ™‚é–“åˆ†æ
        processing_times = [r["processing_time_ms"] for r in successful_results if r["processing_time_ms"] > 0]
        if processing_times:
            avg_processing_time = sum(processing_times) / len(processing_times)
            min_time = min(processing_times)
            max_time = max(processing_times)
        else:
            avg_processing_time = min_time = max_time = 0
        
        # 7. ä¿¡å¿ƒåº¦åˆ†æ
        confidence_scores = [r["confidence_score"] for r in successful_results]
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        min_confidence = min(confidence_scores)
        max_confidence = max(confidence_scores)
        
        # æ‰“å°åˆ†æçµæœ
        self.print_analysis_results(
            successful_results, language_consistency, avg_keyword_count,
            min_keywords, max_keywords, avg_jaccard, repetition_rate,
            top_keywords, avg_processing_time, min_time, max_time,
            avg_confidence, min_confidence, max_confidence,
            exact_match_rate, total_pairs, identical_pairs,
            ci_lower, ci_upper, len(set(keyword_combinations)),
            language_counter
        )
        
        # ç”Ÿæˆå ±å‘Š
        await self.generate_report(
            successful_results, language_consistency, avg_jaccard,
            repetition_rate, top_keywords, exact_match_rate,
            total_pairs, identical_pairs, ci_lower, ci_upper,
            language_counter
        )
    
    def print_analysis_results(self, successful_results, language_consistency, 
                             avg_keyword_count, min_keywords, max_keywords,
                             avg_jaccard, repetition_rate, top_keywords,
                             avg_processing_time, min_time, max_time,
                             avg_confidence, min_confidence, max_confidence,
                             exact_match_rate, total_pairs, identical_pairs,
                             ci_lower, ci_upper, unique_combinations,
                             language_counter):
        """æ‰“å°åˆ†æçµæœ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š TC-416 ä¸€è‡´æ€§åˆ†æçµæœ")
        print("=" * 60)
        
        # èªè¨€çµ±è¨ˆ
        print("ğŸŒ èªè¨€åµæ¸¬çµ±è¨ˆ:")
        for lang, count in language_counter.most_common():
            percentage = (count / len(successful_results)) * 100
            print(f"   {lang}: {count} æ¬¡ ({percentage:.1f}%)")
        
        # KPI æŒ‡æ¨™
        print("\nğŸ¯ æ ¸å¿ƒ KPI æŒ‡æ¨™:")
        print(f"   èªè¨€æª¢æ¸¬ä¸€è‡´æ€§: {language_consistency*100:.1f}% {'âœ…' if language_consistency >= 0.95 else 'âŒ'}")
        print(f"   å®Œå…¨ç›¸åŒç‡: {exact_match_rate*100:.1f}% {'âœ…' if exact_match_rate >= 0.35 else 'âŒ'} (ç›®æ¨™: â‰¥35%)")
        print(f"   å¹³å‡ Jaccard ç›¸ä¼¼åº¦: {avg_jaccard:.3f}")
        
        print(f"\nğŸ“ˆ é—œéµå­—ä¸€è‡´æ€§çµ±è¨ˆ:")
        print(f"   ç¸½é…å°æ•¸: {total_pairs}")
        print(f"   å®Œå…¨ç›¸åŒé…å°æ•¸: {identical_pairs}")
        print(f"   å”¯ä¸€é—œéµå­—çµ„åˆæ•¸: {unique_combinations}")
        print(f"   95% ä¿¡å¿ƒå€é–“: [{ci_lower*100:.1f}%, {ci_upper*100:.1f}%]")
        print(f"   â†’ åœ¨ 95% ä¿¡å¿ƒæ°´æº–ä¸‹ï¼Œä»»å…©æ¬¡å–å¾—ç›¸åŒé—œéµå­—åˆ—è¡¨çš„æ©Ÿç‡ç‚º {ci_lower*100:.1f}% åˆ° {ci_upper*100:.1f}% ä¹‹é–“")
        
        # é—œéµå­—çµ±è¨ˆ
        print(f"\nğŸ“ é—œéµå­—çµ±è¨ˆ:")
        print(f"   å¹³å‡é—œéµå­—æ•¸é‡: {avg_keyword_count:.1f}")
        print(f"   é—œéµå­—æ•¸é‡ç¯„åœ: {min_keywords} - {max_keywords}")
        print(f"   æˆåŠŸæ¸¬è©¦æ•¸é‡: {len(successful_results)}/{self.iterations}")
        
        # æ•ˆèƒ½æŒ‡æ¨™
        print(f"\nâ±ï¸  æ•ˆèƒ½æŒ‡æ¨™:")
        print(f"   å¹³å‡è™•ç†æ™‚é–“: {avg_processing_time:.0f}ms")
        print(f"   è™•ç†æ™‚é–“ç¯„åœ: {min_time}ms - {max_time}ms")
        print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.3f}")
        print(f"   ä¿¡å¿ƒåº¦ç¯„åœ: {min_confidence:.3f} - {max_confidence:.3f}")
        
        # é«˜é »é—œéµå­—
        print(f"\nğŸ”¥ æœ€é«˜é »é—œéµå­— (Top 10):")
        for i, (keyword, freq) in enumerate(top_keywords, 1):
            percentage = (freq / len(successful_results)) * 100
            print(f"   {i:2d}. {keyword} ({freq}æ¬¡, {percentage:.1f}%)")
        
        # æ•´é«”è©•ä¼°
        kpi_pass_count = sum([
            language_consistency >= 0.95,
            exact_match_rate >= 0.35
        ])
        
        print(f"\nğŸ† æ•´é«”è©•ä¼°:")
        print(f"   KPI é€šé: {kpi_pass_count}/2")
        if kpi_pass_count == 2:
            print("   âœ… TC-416 æ¸¬è©¦é€šé - AI ä¸€è‡´æ€§è¡¨ç¾è‰¯å¥½")
        else:
            print("   âŒ TC-416 æ¸¬è©¦æœªé€šé - AI ä¸€è‡´æ€§éœ€è¦æ”¹å–„")
    
    async def generate_report(self, successful_results, language_consistency,
                            avg_jaccard, repetition_rate, top_keywords,
                            exact_match_rate, total_pairs, identical_pairs,
                            ci_lower, ci_upper, language_counter):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        timestamp = int(time.time())
        report_file = f"TC416_consistency_report_{self.detected_language}_{timestamp}.json"
        
        report_data = {
            "test_info": {
                "test_case": "TC-416",
                "test_name": "å¤šèªè¨€é—œéµå­—æå–ä¸€è‡´æ€§æ¸¬è©¦",
                "detected_language": self.detected_language,
                "prompt_version": self.prompt_version_used,
                "execution_date": datetime.now().isoformat(),
                "total_iterations": self.iterations,
                "successful_iterations": len(successful_results),
                "total_execution_time_seconds": self.end_time - self.start_time,
                "jd_content": self.test_jd.strip(),
                "jd_length": len(self.test_jd)
            },
            "language_detection": {
                "statistics": dict(language_counter),
                "consistency_rate": language_consistency
            },
            "kpi_metrics": {
                "language_consistency_rate": language_consistency,
                "exact_match_rate": exact_match_rate,
                "total_pairs": total_pairs,
                "identical_pairs": identical_pairs,
                "confidence_interval_95": {
                    "lower": ci_lower,
                    "upper": ci_upper
                },
                "average_jaccard_similarity": avg_jaccard,
                "keyword_repetition_rate": repetition_rate,
                "kpi_pass_threshold": {
                    "language_consistency": 0.95,
                    "exact_match_rate": 0.35
                }
            },
            "keyword_analysis": {
                "top_keywords": top_keywords,
                "unique_keywords_count": len(set([k for r in successful_results for k in r["keywords"]])),
                "total_keywords_extracted": sum([len(r["keywords"]) for r in successful_results])
            },
            "detailed_results": self.test_results,
            "performance_metrics": {
                "average_processing_time_ms": sum([r["processing_time_ms"] for r in successful_results if r["processing_time_ms"] > 0]) / len([r for r in successful_results if r["processing_time_ms"] > 0]) if any(r["processing_time_ms"] > 0 for r in successful_results) else 0,
                "average_confidence_score": sum([r["confidence_score"] for r in successful_results]) / len(successful_results)
            }
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = project_root / report_file
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°æ¸¬è©¦å ±å‘Šå·²ä¿å­˜:")
        print(f"   æª”æ¡ˆ: {report_path}")
        print(f"   å¤§å°: {os.path.getsize(report_path)} bytes")

def parse_arguments():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(
        description="TC-416: å¤šèªè¨€é—œéµå­—æå–ä¸€è‡´æ€§æ¸¬è©¦",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  %(prog)s                           # ä½¿ç”¨é è¨­è‹±æ–‡ JDï¼Œ50æ¬¡æ¸¬è©¦
  %(prog)s --jd-file chinese.txt    # ä½¿ç”¨æª”æ¡ˆä¸­çš„ JD
  %(prog)s --jd "è·ä½æè¿°å…§å®¹"        # ç›´æ¥æä¾› JD å…§å®¹
  %(prog)s --iterations 20          # åªåŸ·è¡Œ 20 æ¬¡æ¸¬è©¦
  %(prog)s --jd-file job.txt -n 100 # ä½¿ç”¨æª”æ¡ˆï¼ŒåŸ·è¡Œ 100 æ¬¡æ¸¬è©¦
        """
    )
    
    # JD ä¾†æºï¼ˆäº’æ–¥ç¾¤çµ„ï¼‰
    jd_group = parser.add_mutually_exclusive_group()
    jd_group.add_argument(
        '--jd-file', 
        type=str,
        help='å¾æª”æ¡ˆè®€å– JD å…§å®¹'
    )
    jd_group.add_argument(
        '--jd',
        type=str,
        help='ç›´æ¥æä¾› JD å…§å®¹'
    )
    
    # æ¸¬è©¦æ¬¡æ•¸
    parser.add_argument(
        '-n', '--iterations',
        type=int,
        default=50,
        help='æ¸¬è©¦æ¬¡æ•¸ï¼ˆé è¨­: 50ï¼‰'
    )
    
    # å»¶é²æ™‚é–“
    parser.add_argument(
        '-d', '--delay',
        type=float,
        default=2.0,
        help='æ¯æ¬¡æ¸¬è©¦ä¹‹é–“çš„å»¶é²ç§’æ•¸ï¼Œç”¨æ–¼é¿å… rate limitï¼ˆé è¨­: 2.0ï¼‰'
    )
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•¸"""
    args = parse_arguments()
    
    print("ğŸš€ å•Ÿå‹• TC-416 å¤šèªè¨€é—œéµå­—ä¸€è‡´æ€§æ¸¬è©¦")
    print("ğŸ“‹ é€™æ˜¯æ ¸å¿ƒ KPI æ¸¬è©¦ï¼Œå°‡è©•ä¼° AI çš„ç©©å®šæ€§å’Œä¸€è‡´æ€§")
    print("")
    
    # æª¢æŸ¥ç’°å¢ƒ
    try:
        from src.services.keyword_extraction_v2 import KeywordExtractionServiceV2
    except ImportError as e:
        print(f"âŒ ç’°å¢ƒæª¢æŸ¥å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿å·²æ­£ç¢ºè¨­ç½® Python ç’°å¢ƒå’Œä¾è³´å¥—ä»¶")
        return False
    
    # è¨­ç½®ç’°å¢ƒè®Šæ•¸
    os.environ['PYTHONPATH'] = str(project_root) + ":" + os.environ.get('PYTHONPATH', '')
    
    # æ±ºå®šä½¿ç”¨çš„ JD
    if args.jd_file:
        try:
            with open(args.jd_file, 'r', encoding='utf-8') as f:
                test_jd = f.read()
            print(f"ğŸ“„ å¾æª”æ¡ˆè¼‰å…¥ JD: {args.jd_file}")
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–æª”æ¡ˆ {args.jd_file}: {e}")
            return False
    elif args.jd:
        test_jd = args.jd
        print("ğŸ“ ä½¿ç”¨å‘½ä»¤åˆ—æä¾›çš„ JD")
    else:
        test_jd = DEFAULT_JD
        print("ğŸ“ ä½¿ç”¨é è¨­è‹±æ–‡ JD")
    
    # åŸ·è¡Œæ¸¬è©¦
    test = TC416ConsistencyTest(iterations=args.iterations, delay_seconds=args.delay)
    test.set_test_jd(test_jd)
    await test.run_consistency_test()
    
    print("\nâœ¨ TC-416 ä¸€è‡´æ€§æ¸¬è©¦åŸ·è¡Œå®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)