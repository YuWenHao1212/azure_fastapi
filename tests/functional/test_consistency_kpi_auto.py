#!/usr/bin/env python3
"""
ğŸ¯ è‡ªå‹•èªè¨€åµæ¸¬ç‰ˆæœ¬çš„ä¸€è‡´æ€§ KPI æ¸¬è©¦
æœƒæ ¹æ“š JD å…§å®¹è‡ªå‹•åµæ¸¬èªè¨€ï¼Œè€Œéå¼·åˆ¶ä½¿ç”¨æŒ‡å®šèªè¨€

ä½¿ç”¨æ–¹å¼ï¼š
    python test_consistency_kpi_auto.py              # ä½¿ç”¨é è¨­ä¸­æ–‡ JD
    python test_consistency_kpi_auto.py 1.3.0        # æŒ‡å®šç‰ˆæœ¬
    python test_consistency_kpi_auto.py 1.3.0 jd.txt # æŒ‡å®š JD æª”æ¡ˆ
"""

import asyncio
import httpx
import sys
import os
import math
import json
from datetime import datetime
from collections import Counter

# é è¨­çš„æ¸¬è©¦ JDï¼ˆä¸­æ–‡ï¼‰
DEFAULT_JD = """
Established in 1987 and headquartered in Taiwan, TSMC pioneered the pure-play foundry business model with an exclusive focus on manufacturing its customersâ€™ products. In 2023, the company served 528 customers with 11,895 products for high performance computing, smartphones, IoT, automotive, and consumer electronics, and is the worldâ€™s largest provider of logic ICs with annual capacity of 16 million 12-inch equivalent wafers. TSMC operates fabs in Taiwan as well as manufacturing subsidiaries in Washington State, Japan and China, and its ESMC subsidiary plans to begin construction on a fab in Germany in 2024. In Arizona, TSMC is building three fabs, with the first starting 4nm production in 2025, the second by 2028, and the third by the end of the decade.

The Sr. HR Data Analyst will be responsible for analyzing large sets of HR data in order to provide insights and recommendations to the HR team and senior management independently or with other HR analysts across global HR team. The role will require a high level of technical expertise in data visualization and analysis, as well as a deep understanding of HR processes and policies.

**Job Responsibilities:**

1. Act as a Tableau data visualization expert and develop strategic HR dashboards with other domain experts in cross-team projects that enables data-informed decisions to stakeholders.
2. Provide guidance, training plans and technical support to other analysts in HR team for Tableau skills development.
3. Translate business needs into technical data requirements and work with IT data platform engineers for data preparation.
4. Create trust and maintain strong relationships with key stakeholders across the organization.
5. Stay up-to-date with industry trends and new analytics features as a technical advocate.

**Job Qualifications:**

1. Master's degree in HR, Business, Statistics, CS or related field.
2. Minimum 5 years of experience in data analysis, with a proven track record of delivering actionable business insights and recommendations.
3. Strong technical skills in data analysis tools such as Tableau, Power BI, Superset and SQL.
4. Excellent communication skills with the ability to effectively communicate complex data insights to non-technical stakeholders.
5. Strong problem-solving skills with the ability to think critically and creatively to solve complex business problems.
6. Ability to work independently and manage multiple projects simultaneously.
7. Strong attention to details and accuracy.
8. Experience in leading and mentoring junior analysts is a plus

"""

async def test_consistency_kpi(prompt_version: str = "1.3.0", job_description: str = None, num_tests: int = 20):
    """
    åŸ·è¡Œä¸€è‡´æ€§ KPI æ¸¬è©¦ï¼ˆä½¿ç”¨è‡ªå‹•èªè¨€åµæ¸¬ï¼‰
    """
    base_url = "http://localhost:8000/api/v1"
    
    # ä½¿ç”¨æä¾›çš„ JD æˆ–é è¨­å€¼
    jd_to_test = job_description or DEFAULT_JD
    jd_preview = jd_to_test[:100] + "..." if len(jd_to_test) > 100 else jd_to_test
    
    print(f"ğŸ¯ ä¸€è‡´æ€§ KPI æ¸¬è©¦ï¼ˆè‡ªå‹•èªè¨€åµæ¸¬ç‰ˆï¼‰")
    print("=" * 80)
    print(f"Prompt ç‰ˆæœ¬: {prompt_version}")
    print(f"èªè¨€è¨­å®š: auto (è‡ªå‹•åµæ¸¬)")
    print(f"æ¸¬è©¦æ¬¡æ•¸: {num_tests}")
    print(f"JD é è¦½: {jd_preview}")
    print("=" * 80)
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        # å„²å­˜æ‰€æœ‰çµæœ
        all_results = []
        successful_runs = 0
        detected_languages = []
        
        print(f"\nğŸ“Š åŸ·è¡Œ {num_tests} æ¬¡æ¸¬è©¦...")
        
        for run in range(num_tests):
            request_data = {
                "job_description": jd_to_test,
                "max_keywords": 16,
                "prompt_version": prompt_version,
                "language": "auto"  # ä½¿ç”¨è‡ªå‹•åµæ¸¬
            }
            
            try:
                response = await client.post(
                    f"{base_url}/extract-jd-keywords",
                    json=request_data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    keywords = result["data"]["keywords"]
                    detected_lang = result["data"].get("detected_language", "unknown")
                    
                    all_results.append(keywords)
                    detected_languages.append(detected_lang)
                    successful_runs += 1
                    
                    print(f"   Run {run+1:2d}: âœ“ ({len(keywords)} keywords, lang={detected_lang})")
                else:
                    print(f"   Run {run+1:2d}: âœ— (Status {response.status_code})")
                    
            except Exception as e:
                print(f"   Run {run+1:2d}: âœ— (Error: {str(e)})")
            
            # å»¶é²é¿å… rate limiting
            await asyncio.sleep(1.5)
        
        # åˆ†æçµæœ
        if successful_runs < 2:
            print("\nâŒ æ¸¬è©¦å¤±æ•—ï¼šæˆåŠŸæ¬¡æ•¸ä¸è¶³")
            return
        
        print(f"\nâœ… æˆåŠŸåŸ·è¡Œ: {successful_runs}/{num_tests} æ¬¡")
        
        # èªè¨€åµæ¸¬çµ±è¨ˆ
        lang_counter = Counter(detected_languages)
        print(f"\nğŸŒ èªè¨€åµæ¸¬çµ±è¨ˆ:")
        for lang, count in lang_counter.most_common():
            print(f"   {lang}: {count} æ¬¡ ({count/successful_runs*100:.1f}%)")
        
        # é—œéµå­—åˆ†æ
        if all_results:
            # çµ±è¨ˆæ‰€æœ‰é—œéµå­—å‡ºç¾é »ç‡
            keyword_counter = Counter()
            for result in all_results:
                keyword_counter.update(result)
            
            print(f"\nğŸ“Š é—œéµå­—é »ç‡åˆ†æ:")
            print(f"   ç¸½å…±å‡ºç¾ {len(keyword_counter)} å€‹ä¸åŒé—œéµå­—")
            
            # é¡¯ç¤ºå‰ 20 å€‹æœ€å¸¸å‡ºç¾çš„é—œéµå­—
            print(f"\nğŸ” å‰ 20 å€‹é«˜é »é—œéµå­—:")
            for i, (keyword, count) in enumerate(keyword_counter.most_common(20), 1):
                percentage = (count / successful_runs) * 100
                print(f"   {i:2d}. {keyword}: {count} æ¬¡ ({percentage:.1f}%)")
            
            # è¨ˆç®—ä¸€è‡´æ€§å’Œç›¸ä¼¼åº¦
            total_pairs = successful_runs * (successful_runs - 1) // 2
            identical_pairs = 0
            jaccard_scores = []
            
            for i in range(len(all_results)):
                for j in range(i + 1, len(all_results)):
                    set1 = set(all_results[i])
                    set2 = set(all_results[j])
                    
                    # æª¢æŸ¥æ˜¯å¦å®Œå…¨ç›¸åŒ
                    if sorted(all_results[i]) == sorted(all_results[j]):
                        identical_pairs += 1
                    
                    # è¨ˆç®— Jaccard ç›¸ä¼¼åº¦
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)
                    if union > 0:
                        jaccard = intersection / union
                        jaccard_scores.append(jaccard)
            
            consistency_rate = identical_pairs / total_pairs if total_pairs > 0 else 0
            
            print(f"\nğŸ“Š ä¸€è‡´æ€§ KPI åˆ†æ:")
            print(f"   ç¸½é…å°æ•¸: {total_pairs}")
            print(f"   ç›¸åŒé…å°æ•¸: {identical_pairs}")
            print(f"   ä¸€è‡´æ€§ç‡: {consistency_rate:.1%}")
            
            # è¨ˆç®— 95% ä¿¡å¿ƒå€é–“
            # ä½¿ç”¨äºŒé …åˆ†ä½ˆçš„æ­£æ…‹è¿‘ä¼¼
            import math
            
            if total_pairs > 0:
                p = consistency_rate
                n = total_pairs
                
                # æ¨™æº–èª¤å·®
                if p > 0 and p < 1:
                    se = math.sqrt(p * (1 - p) / n)
                else:
                    se = 0
                
                # 95% ä¿¡å¿ƒå€é–“ (z = 1.96)
                z = 1.96
                margin_of_error = z * se
                ci_lower = max(0, p - margin_of_error)
                ci_upper = min(1, p + margin_of_error)
                
                print(f"\nğŸ“ˆ çµ±è¨ˆåˆ†æ:")
                print(f"   95% ä¿¡å¿ƒå€é–“: [{ci_lower:.1%}, {ci_upper:.1%}]")
                print(f"   æ¨™æº–èª¤å·®: {se:.3f}")
                print(f"   èª¤å·®ç¯„åœ: Â±{margin_of_error:.1%}")
                
                # è§£é‡‹ä¿¡å¿ƒå€é–“
                print(f"\nğŸ’¡ è§£é‡‹:")
                print(f"   åœ¨ 95% çš„ä¿¡å¿ƒæ°´æº–ä¸‹ï¼Œä»»å…©æ¬¡åŸ·è¡Œå–å¾—ç›¸åŒé—œéµå­—åˆ—è¡¨çš„æ©Ÿç‡ç‚º:")
                print(f"   {ci_lower:.1%} åˆ° {ci_upper:.1%} ä¹‹é–“")
                
                # è¨ˆç®—é”åˆ°ç›®æ¨™æ‰€éœ€çš„æ¨£æœ¬æ•¸
                target_rate = 0.35
                if consistency_rate < target_rate:
                    # ä½¿ç”¨ç•¶å‰çš„è®Šç•°æ€§ä¼°ç®—éœ€è¦å¤šå°‘æ¬¡æ¸¬è©¦
                    required_identical = math.ceil(target_rate * total_pairs)
                    print(f"\nğŸ“Š é”æ¨™åˆ†æ:")
                    print(f"   ç›®æ¨™ä¸€è‡´æ€§ç‡: {target_rate:.0%}")
                    print(f"   éœ€è¦ç›¸åŒé…å°æ•¸: {required_identical} (ç›®å‰: {identical_pairs})")
                    print(f"   å·®è·: {required_identical - identical_pairs} å°")
            
            # KPI åˆ¤å®š
            target_rate = 0.35  # 35% ç›®æ¨™å€¼
            if consistency_rate >= target_rate:
                print(f"\nğŸ¯ KPI è©•ä¼°: âœ… é€šéï¼({consistency_rate:.1%} â‰¥ 35%)")
            else:
                print(f"\nğŸ¯ KPI è©•ä¼°: âŒ æœªé”æ¨™ï¼({consistency_rate:.1%} < 35%)")
            
            # Jaccard ç›¸ä¼¼åº¦åˆ†æ
            if jaccard_scores:
                avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)
                min_jaccard = min(jaccard_scores)
                max_jaccard = max(jaccard_scores)
                
                # è¨ˆç®— Jaccard åˆ†æ•¸çš„æ¨™æº–å·®
                jaccard_variance = sum((x - avg_jaccard) ** 2 for x in jaccard_scores) / len(jaccard_scores)
                jaccard_std = math.sqrt(jaccard_variance)
                
                print(f"\nğŸ“Š Jaccard ç›¸ä¼¼åº¦åˆ†æ:")
                print(f"   å¹³å‡ç›¸ä¼¼åº¦: {avg_jaccard:.1%}")
                print(f"   æœ€ä½ç›¸ä¼¼åº¦: {min_jaccard:.1%}")
                print(f"   æœ€é«˜ç›¸ä¼¼åº¦: {max_jaccard:.1%}")
                print(f"   æ¨™æº–å·®: {jaccard_std:.3f}")
                
                # åˆ†çµ„çµ±è¨ˆ
                high_similarity = sum(1 for s in jaccard_scores if s >= 0.8)
                medium_similarity = sum(1 for s in jaccard_scores if 0.5 <= s < 0.8)
                low_similarity = sum(1 for s in jaccard_scores if s < 0.5)
                
                print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
                print(f"   é«˜ç›¸ä¼¼åº¦ (â‰¥80%): {high_similarity} å° ({high_similarity/len(jaccard_scores)*100:.1f}%)")
                print(f"   ä¸­ç›¸ä¼¼åº¦ (50-79%): {medium_similarity} å° ({medium_similarity/len(jaccard_scores)*100:.1f}%)")
                print(f"   ä½ç›¸ä¼¼åº¦ (<50%): {low_similarity} å° ({low_similarity/len(jaccard_scores)*100:.1f}%)")
            
            # å„²å­˜è©³ç´°çµæœåˆ° JSON
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"consistency_test_results_{timestamp}.json"
            
            results = {
                "test_info": {
                    "timestamp": datetime.now().isoformat(),
                    "prompt_version": prompt_version,
                    "language": "auto",
                    "num_tests": num_tests,
                    "successful_runs": successful_runs,
                    "jd_preview": jd_preview
                },
                "consistency_analysis": {
                    "total_pairs": total_pairs,
                    "identical_pairs": identical_pairs,
                    "consistency_rate": consistency_rate,
                    "confidence_interval_95": {
                        "lower": ci_lower if 'ci_lower' in locals() else None,
                        "upper": ci_upper if 'ci_upper' in locals() else None,
                        "margin_of_error": margin_of_error if 'margin_of_error' in locals() else None
                    }
                },
                "jaccard_analysis": {
                    "average": avg_jaccard if jaccard_scores else None,
                    "min": min_jaccard if jaccard_scores else None,
                    "max": max_jaccard if jaccard_scores else None,
                    "std_dev": jaccard_std if jaccard_scores else None,
                    "distribution": {
                        "high": high_similarity if jaccard_scores else 0,
                        "medium": medium_similarity if jaccard_scores else 0,
                        "low": low_similarity if jaccard_scores else 0
                    }
                },
                "keyword_frequency": dict(keyword_counter.most_common()),
                "language_detection": dict(lang_counter),
                "all_results": all_results,
                "kpi_status": "PASSED" if consistency_rate >= target_rate else "FAILED"
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ è©³ç´°çµæœå·²å„²å­˜è‡³: {output_file}")

async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    # è§£æå‘½ä»¤åˆ—åƒæ•¸
    prompt_version = "1.3.0"  # é è¨­ç‰ˆæœ¬
    job_description = None
    
    if len(sys.argv) > 1:
        prompt_version = sys.argv[1]
    
    if len(sys.argv) > 2:
        # å¾æª”æ¡ˆè®€å– JD
        jd_file = sys.argv[2]
        if jd_file and os.path.exists(jd_file):
            with open(jd_file, 'r', encoding='utf-8') as f:
                job_description = f.read()
            print(f"ğŸ“„ å¾æª”æ¡ˆè¼‰å…¥ JD: {jd_file}")
    
    # æª¢æŸ¥ä¼ºæœå™¨æ˜¯å¦é‹è¡Œ
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:8000/api/v1/health")
            if response.status_code != 200:
                print("âŒ API ä¼ºæœå™¨æœªå›æ‡‰")
                return
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° API ä¼ºæœå™¨: {str(e)}")
        print("\nè«‹å…ˆå•Ÿå‹• API ä¼ºæœå™¨:")
        print("uvicorn src.main:app --reload --port 8000")
        return
    
    # åŸ·è¡Œæ¸¬è©¦
    await test_consistency_kpi(prompt_version, job_description)

if __name__ == "__main__":
    asyncio.run(main())